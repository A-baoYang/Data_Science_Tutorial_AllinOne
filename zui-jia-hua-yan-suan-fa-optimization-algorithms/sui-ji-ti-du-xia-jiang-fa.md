# 梯度下降法 Gradient Descent

## 梯度下降法 Gradient Descent

* 「求解機器學習算法的模型參數」，屬於無約束優化問題
* 梯度向量的幾何意義：函數變化增加最快的地方 => 沿著梯度的方向，更容易找到函數的最大值；沿相反方向，則越容易找到函數最小值
  * 最小化損失函數：透過 梯度下降法 迭代求解
  * 最大化：梯度上升法（其實就是反過來求解 $$-f(\theta)$$）
* 如果損失函數是[凸函數](https://zh.wikipedia.org/wiki/%E5%87%B8%E5%87%BD%E6%95%B0)，梯度下降則一定能夠找到全域最佳解；如果不是，則不一定，可能找到的是局部最佳解

### 相關函式及參數

#### 學習率 Learning Rate

* 一次函數下降迭代的變化量（just like 爬山的步長）

#### 假設函數 Hypothesis function

* 用來擬合真實樣本，又稱擬合函數

#### 損失函數 Loss Function

* 用來評估模型擬合 (fitting) 情形，損失函數結果越小表示擬合效果越好；對應找到的模型參數即為最佳參數

### 算法調參

#### **參數初始值選擇**

* 如果損失函數不是凸函數，會因為初始值不同，影響優先找到的最佳解是否為全域最佳；因此**要取不同初始值跑多次梯度下降算法，來選擇最好的初始值**

#### **學習率選擇**

* 根據 loss function 結果搜尋最佳選項，例如：網格搜索

#### **特徵標準化**

* 避免樣本的特徵範圍不同，影響梯度下降的迭代速度，將每個特徵縮放到同一範圍，例如：標準化（統一期望值為 0、標準差為 1）、最大最小化等

##
